# -*- coding: utf-8 -*-
"""Learn_NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Xe8wW_4dBZiokvOoEYgTCIufBIF4fkR

The utilization of the `langdetect` library for identifying English comments enhances the code's functionality by enabling language detection capabilities. By checking for English comments, the code ensures that only relevant data is processed further, improving the accuracy and efficiency of subsequent analyses.
"""

# !pip install langdetect
!pip install langdetect

"""<div class="alert alert-block alert-info">
    
## Table of Contents

</div>

[1. Introduction](#Intro) <br>
[2. Importing Libraries](#libs) <br>
[3. Examining Input File](#examine) <br>
[4. Loading and Parsing Files](#load) <br>
$\;\;\;\;$[4.1. Tokenization](#tokenize) <br>
$\;\;\;\;$[4.2. Whatever else](#whetev) <br>
$\;\;\;\;$[4.3. Genegrate numerical representation](#whetev1) <br>
[5. Writing Output Files](#write) <br>
$\;\;\;\;$[5.1. Vocabulary List](#write-vocab) <br>
$\;\;\;\;$[5.2. Sparse Matrix](#write-sparseMat) <br>
[6. Summary](#summary) <br>
[7. References](#Ref) <br>

<div class="alert alert-block alert-success">
    
## 1.  Introduction  <a class="anchor" name="Intro"></a>

This task involves preprocessing YouTube comments stored in an Excel file, converting them into numerical representations for downstream modeling tasks like recommender systems or machine translation. Utilizing Python, the code iterates through the comments, converting words into numbers, a fundamental step in natural language processing. This numerical representation enables machines to understand and decode language patterns, essential for machine learning algorithms. Through this iterative process, the code lays the groundwork for feature selection in subsequent modeling endeavors. All libraries are permitted for use in this task.

<div class="alert alert-block alert-success">
    
## 2.  Importing Libraries  <a class="anchor" name="libs"></a>

In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:

* **os:** to interact with the operating system, e.g. navigate through folders to read files
* **re:** to define and use regular expressions
* **pandas:** to work with dataframes
* **multiprocessing:** to perform processes on multi cores for fast performance
* **nltk:** to perform langiage processing
* **langdetect:** to detect english comments
* **openpyxl:** to manupulayte excel files
"""

import os
import re
# import langid
import pandas as pd
import multiprocessing
from itertools import chain
import nltk
from nltk.probability import *
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import MWETokenizer
from nltk.stem import PorterStemmer
from nltk.util import ngrams
from openpyxl import load_workbook

"""-------------------------------------

<div class="alert alert-block alert-success">
    
## 3.  Examining Input File <a class="anchor" name="examine"></a>

3.1

The code snippet utilizes the Google Colab library to mount Google Drive, providing access to files and directories stored in the user's Google Drive account. This functionality facilitates seamless integration between Google Colab notebooks and Google Drive, enabling users to read from and write to files stored in their Drive.
"""

from google.colab import drive
drive.mount('/content/drive')

"""3.2

The code snippet imports the Pandas library and reads an Excel file located at the specified path ('file_path.xlsx') into a dictionary of DataFrames. Each sheet in the Excel file is represented as a key-value pair in the dictionary, with the sheet name as the key and the corresponding DataFrame as the value. It then iterates over the dictionary items, printing the name of each sheet followed by the contents of the associated DataFrame. This allows users to inspect the data from each sheet within the Excel file.
"""

import pandas as pd

# Replace 'file_path.xlsx' with the path to your Excel file
file_path = '/content/drive/My Drive/Group037.xlsx'

# Read all sheets into a dictionary of DataFrames
dfs = pd.read_excel(file_path, sheet_name=None)

# Iterate over the sheets and display each DataFrame
for sheet_name, df in dfs.items():
    print(f"Sheet Name: {sheet_name}")
    print(df)  # Display the DataFrame

"""id and snippets, but they are not at the first row of excel sheets

3.3

This code snippet employs the `openpyxl` library to manage Excel files in Python, focusing on data cleaning and consolidation tasks. It starts by loading an Excel workbook from the specified file path and iterates through each sheet to extract 'id' and 'snippet' data. Subsequently, it consolidates all IDs and snippets across sheets, merging them into unified lists while removing duplicate entries. Furthermore, it performs cleaning operations on the extracted snippets, removing unnecessary slashes to enhance data integrity. This systematic approach streamlines data preprocessing, ensuring that the extracted information is standardized and ready for subsequent analyses or downstream tasks. By incorporating these cleaning and consolidation steps, the code facilitates efficient data management and analysis workflows within Python environments.
"""

file_path = '/content/drive/My Drive/Group037.xlsx'

# Load the workbook
workbook = load_workbook(file_path)

all_ids = []
all_snippets = []

# Iterate over each sheet in the workbook
for sheet_name in workbook.sheetnames:
    # Get the current sheet
    sheet = workbook[sheet_name]

    # Iterate over each row in the sheet
    for row in sheet.iter_rows():
        for cell in row:
            # Check if the cell name is 'id'
            if cell.value == 'id':
                # Collect the ID and its values
                id_info = [cell.value]
                for i in range(1, sheet.max_row - cell.row):  # Get all values below the ID cell
                    value = sheet.cell(row=cell.row + i, column=cell.column).value
                    if value is not None:
                        id_info.append(value)
                all_ids.append(id_info)

            # Check if the cell name is 'snippet'
            elif cell.value == 'snippet':
                # Collect the snippet and its values
                snippet_info = [cell.value]
                for i in range(1, sheet.max_row - cell.row):  # Get all values below the snippet cell
                    value = sheet.cell(row=cell.row + i, column=cell.column).value
                    if value is not None:
                        snippet_info.append(value)
                all_snippets.append(snippet_info)
#uniqueness check

# unique_ids = set()
# unique_merged_ids = []
# for id_value in merged_ids:
#     if id_value not in unique_ids:
#         unique_ids.add(id_value)
#         unique_merged_ids.append(id_value)





# Print all collected IDs
print("All IDs:")
for id_info in all_ids:
    print(len(id_info))

# # Print all collected snippets
print("\nAll Snippets:")
for snippet_info in all_snippets:
    print(len(snippet_info))

merged_ids = []
for id_info in all_ids:
    merged_ids.extend(id_info[1:])

# Merge all snippets from all sheets
merged_snippets = []
for snippet_info in all_snippets:
    merged_snippets.extend(snippet_info[1:])

unique_ids = set()
unique_merged_ids = []
unique_merged_snippets = []
for id_value, snippet in zip(merged_ids, merged_snippets):
    if id_value not in unique_ids:
        unique_ids.add(id_value)
        unique_merged_ids.append(id_value)
        unique_merged_snippets.append(snippet)

# Print the merged IDs and snippets
print("Merged IDs:")
print(len(merged_ids))

print("\nMerged Snippets:")
print(len(merged_snippets))

#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

print("Unique Merged IDs:")
print(len(unique_merged_ids))
print((unique_merged_ids[:1]))
print((unique_merged_ids[-1:]))

print("\nUnique Merged Snippets:")
print(len(unique_merged_ids))
print((unique_merged_snippets[:1]))
print((unique_merged_snippets[-1:]))

cleaned_snippets = []

for snippet in unique_merged_snippets:
    # Remove unnecessary slashes from each dictionary string
    cleaned_snippet = snippet.replace('\\', '')
    cleaned_snippets.append(cleaned_snippet)

# Print cleaned snippets
for snippet in cleaned_snippets[:1]:
    print(snippet)



"""<div class="alert alert-block alert-success">
    
## 4.  Loading and Parsing File <a class="anchor" name="load"></a>

In this section, we will be generating a csv file which will contain channel id, number of comments on that channel and number of english counts per channel.

4.1

Extracting text orignal from all the snippets. This code snippet employs the `eval` function to parse each snippet string as a dictionary, facilitating access to its key-value pairs. It utilizes the `.get()` method to safely retrieve values from nested dictionaries, ensuring robustness against missing or malformed data. Additionally, the `enumerate` function iterates over the processed snippets, providing both the index and the snippet content in each iteration. Exception handling is implemented to catch and handle any errors that may occur during parsing. After extraction, the 'textOriginal' content and corresponding 'channelId' are printed alongside their enumeration indices for clarity and verification. This systematic approach ensures accurate data extraction, with the output displaying both the 'channelId' and 'textOriginal' for each snippet.
"""

#initiating lists to store text original and channel ids
all_text_original = []
all_channel_ids = []

for snippet_str in cleaned_snippets:
    try:
        # Parse the snippet string as a dictionary
        snippet_dict = eval(snippet_str)

        # Extract the 'textOriginal' field from the snippet dictionary
        text_original = snippet_dict.get('topLevelComment', {}).get('snippet', {}).get('textOriginal', '')

        # Extract the 'channelId' field from the snippet dictionary
        channel_id = snippet_dict.get('channelId', '')

        # Append the 'textOriginal' content to the all_text_original list
        all_text_original.append(text_original)

        # Append the 'channelId' to the all_channel_ids list
        all_channel_ids.append(channel_id)
    except Exception as e:
        print(f"Error processing snippet: {e}")

# Print the content of textOriginal from all snippets
if all_text_original:
    for i, (snippet_text, channel_id) in enumerate(zip(all_text_original, all_channel_ids), 1):
        print(f" ChannelId: {channel_id}: TextOriginal: {snippet_text}")
else:
    print("No 'textOriginal' content found in snippets.")

# print(len(all_text_original))
# print(len(all_channel_ids))

# from langdetect import detect

"""4.2

This script facilitates the removal of emojis from a list of YouTube comments. It begins by loading a set of emojis from a specified file path. The `remove_emojis` function applies a regular expression pattern to match and replace emojis within each comment, resulting in a cleaned version of the text. Additionally, it includes a `decode_comments` function to handle byte-string comments by decoding them into UTF-8 format. The decoded comments are then processed to remove emojis. The output displays the cleaned comments alongside their corresponding index numbers, ensuring transparency and clarity in the cleaning process.
"""

''' Remove Emojis'''

import re

def load_emojis(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        emojis = file.read().split()
    return emojis

def remove_emojis(comment, emojis):
    emoji_pattern = "|".join(re.escape(e) for e in emojis)
    clean_comment = re.sub(emoji_pattern, '', comment)
    return clean_comment

def decode_comments(comments):
    decoded_comments = []
    for comment in comments:
        if isinstance(comment, bytes):
            try:
                decoded_comment = comment.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    decoded_comment = comment.decode('unicode_escape')
                except UnicodeDecodeError:
                    # Handle the exception by skipping the comment
                    print(f"Failed to decode comment: {comment}")
                    continue
            decoded_comments.append(decoded_comment)
        else:
            decoded_comments.append(comment)
    return decoded_comments

# Load emojis
emoji_file_path = '/content/drive/My Drive/emoji.txt'
emojis = load_emojis(emoji_file_path)

# Sample list of comments (replace this with your actual list of comments)


# Decode comments if they are byte strings
youtube_comments = decode_comments(all_text_original)

# Remove emojis from comments
cleaned_comments = []
for comment in youtube_comments:
    cleaned_comment = remove_emojis(comment, emojis)
    cleaned_comments.append(cleaned_comment)

for i, comment in enumerate(cleaned_comments, 1):
    print(f"Comment {i}: {comment}")

"""4.3

This script processes a list of YouTube comments, identifying English comments and categorizing them based on their associated channel IDs. It utilizes the `detect_english_comments` function to detect and collect English comments using the `langdetect` library. The counts of total comments and English comments are stored in dictionaries, indexed by channel IDs. Finally, the results are written to a CSV file containing three columns: Channel ID, Total Count, and English Count. This CSV file serves as a comprehensive record of the comment data, facilitating further analysis and insights.
"""

''' CSV file'''



from collections import defaultdict
from langdetect import detect

# Function to detect English comments
def detect_english_comments(comments):
    english_comments = []
    for comment in comments:
        try:
            lang = detect(comment)
            if lang == 'en':
                english_comments.append(comment)
        except:
            continue
    return english_comments

# Initialize dictionaries to store counts for each channel ID
channel_id_counts = defaultdict(int)
english_counts = defaultdict(int)
eng_com = []  # List to store all English comments

# Loop through all channel IDs and text originals
for channel_id, text_original in zip(all_channel_ids, cleaned_comments):
    # Increment count for this channel ID
    channel_id_counts[channel_id] += 1

    # Detect English comments
    english_comments = detect_english_comments([text_original])
    # Increment English count for this channel ID
    english_counts[channel_id] += len(english_comments)

    # Store English comments in the list
    eng_com.extend(english_comments)

# Write results to CSV file


# Print the results
for channel_id, count in channel_id_counts.items():
    english_count = english_counts[channel_id]
    print(f"Channel ID: {channel_id}, Count: {count}, English Count: {english_count}")

# # Printing all English comments
# print("All English Comments:")
# for comment in eng_com:
#     print(comment)

import csv

# Write results to CSV file
csv_file_path = '/content/drive/My Drive/037_channel_lists.csv'
with open(csv_file_path, 'w', newline='') as csvfile:
    fieldnames = ['Channel ID', 'Total Count', 'English Count']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    for channel_id, count in channel_id_counts.items():
        english_count = english_counts[channel_id]
        writer.writerow({'Channel ID': channel_id, 'Total Count': count, 'English Count': english_count})

print("CSV file generated successfully at:", csv_file_path)



"""<div class="alert alert-block alert-warning">
    
### 5. Tokenization <a class="anchor" name="tokenize"></a>

5.1

This script further processes the English comments, filtering out those associated with channels that have fewer than 15 English comments. It removes context-independent words using a list of predefined stop words. The filtered comments are stored in a list called `ci_comments`. For each filtered channel ID, it prints the total count of comments, the count of English comments, and the first three comments as samples. This filtering step ensures that only channels with a substantial number of English comments are considered, providing a more robust dataset for analysis.
"""

# print(len(eng_com))

'''Filtering out Channels less than 15 english comments and removing context indepenedent'''


from collections import defaultdict

# Function to load stop words from file
def load_stopwords(file_path):
    with open(file_path, 'r') as file:
        stopwords = {line.strip() for line in file}
    return stopwords

ci_comments = []  # List to store all context_independent comments

# Load stop words
stopwords = load_stopwords('/content/drive/My Drive/stopwords_en.txt')

# Initialize dictionaries to store counts for each channel ID
channel_id_counts = defaultdict(int)
english_counts = defaultdict(int)
channel_comments = defaultdict(list)  # Dictionary to store comments for each channel

# Loop through all channel IDs and text originals
for channel_id, text_original in zip(all_channel_ids, eng_com):
    # Increment count for this channel ID
    channel_id_counts[channel_id] += 1

    # Tokenize text and filter out stop words
    tokens = [token for token in text_original.split() if token.lower() not in stopwords]

    # Join tokens back into text
    text_without_stopwords = ' '.join(tokens)

    # Count English comments after stop words removal
    english_comments = detect_english_comments([text_without_stopwords])
    english_counts[channel_id] += len(english_comments)
    # Store English comments in the list
    ci_comments.extend(english_comments)

    # Store comments for each channel
    channel_comments[channel_id].append(text_without_stopwords)

# Filter out channel IDs with less than 15 English comments
filtered_channel_ids = [channel_id for channel_id, english_count in english_counts.items() if english_count >= 15]

# Print three comments for each filtered channel ID
for channel_id in filtered_channel_ids:
    count = channel_id_counts[channel_id]
    english_count = english_counts[channel_id]
    print(f"Channel ID: {channel_id}, Count: {count}, English Count: {english_count}")
    print("Comments:")
    for comment in channel_comments[channel_id][:3]:  # Print first three comments
        print(comment)
    print()  # Empty line for separation

# # Printing all English comments
# print("All English Comments:")
# for comment in ci_comments:
#     print(comment)

"""5.3

This script continues processing the English comments, now focusing on context-dependent words. It calculates the frequency of each word across all comments and identifies context-dependent stopwords by comparing their frequencies against a threshold. Comments are then tokenized and filtered to remove both common stopwords and context-dependent stopwords. The remaining comments are stored in a list called `cd_comments`. For each filtered channel ID, it prints the total count of comments, the count of English comments, and the first three comments as samples. Additionally, it prints the words that were removed as context-dependent stopwords.
"""

''' Conetxt Dependent'''

from collections import defaultdict, Counter

cd_comments = []  # List to store all context_dependent comments

# Function to calculate word frequencies across all comments
def calculate_word_frequencies(comments):
    word_counts = Counter()
    for comment in comments:
        words = comment.split()
        word_counts.update(words)
    return word_counts

# Load English stop words
stopwords = load_stopwords('/content/drive/My Drive/stopwords_en.txt')

# Calculate word frequencies across all comments
all_word_frequencies = calculate_word_frequencies(ci_comments)

# Calculate the threshold frequency for context-dependent stopwords
total_channel_ids = len(set(all_channel_ids))
threshold_frequency = total_channel_ids * 0.99

# Identify context-dependent stopwords
context_dependent_stopwords = {word for word, freq in all_word_frequencies.items() if freq > threshold_frequency}

# Initialize dictionaries to store counts for each channel ID
channel_id_counts = defaultdict(int)
english_counts = defaultdict(int)
channel_comments = defaultdict(list)  # Dictionary to store comments for each channel
removed_words = set()  # Set to store removed words

# Loop through all channel IDs and text originals
for channel_id, text_original in zip(all_channel_ids, ci_comments):
    # Increment count for this channel ID
    channel_id_counts[channel_id] += 1

    # Tokenize text and filter out stop words
    tokens = []
    for token in text_original.split():
        token_lower = token.lower()
        if token_lower in stopwords or token_lower in context_dependent_stopwords:
            removed_words.add(token_lower)  # Add removed word to the set
        else:
            tokens.append(token)

    # Join tokens back into text
    text_without_stopwords = ' '.join(tokens)

    # Count English comments after stop words removal
    english_comments = detect_english_comments([text_without_stopwords])
    english_counts[channel_id] += len(english_comments)

    # Store English comments in the list
    cd_comments.extend(english_comments)


    # Store comments for each channel
    channel_comments[channel_id].append(text_without_stopwords)

# Filter out channel IDs with less than 15 English comments
filtered_channel_ids = [channel_id for channel_id, english_count in english_counts.items() if english_count >= 15]

# Print three comments for each filtered channel ID
for channel_id in filtered_channel_ids:
    count = channel_id_counts[channel_id]
    english_count = english_counts[channel_id]
    print(f"Channel ID: {channel_id}, Count: {count}, English Count: {english_count}")
    print("Comments:")
    for comment in channel_comments[channel_id][:3]:  # Print first three comments
        print(comment)
    print()  # Empty line for separation

# Print removed words
print("Removed Words:")
print(removed_words)

# Printing all English comments
print("All English Comments:")
for comment in cd_comments:
    print(comment)

"""5.4

In this step, the script focuses on handling rare tokens and stemming. It first tokenizes each comment using NLTK and applies stemming using the Porter stemmer. Tokens shorter than three characters and common stopwords are filtered out. The script then calculates token frequencies and filters out rare tokens based on a threshold frequency. Next, it filters out channel IDs with fewer than 15 English comments. For each filtered channel ID, it prints the total count of comments, the count of English comments, and the first three comments with rare tokens filtered out. Finally, it prints the stemmed and filtered comments stored in the `stem_com` list.
"""

''' STEP 6 Rare Token and stemming  '''

import re
import nltk
from nltk.stem import PorterStemmer
from collections import defaultdict

# Download NLTK resources (run only once)
nltk.download('punkt')

# Function to load stop words from file
def load_stopwords(file_path):
    with open(file_path, 'r') as file:
        stopwords = {line.strip() for line in file}
    return stopwords

# Function to detect English comments (dummy implementation)
def detect_english_comments(comments):
    return [comment for comment in comments if comment]

# Load English stop words
stopwords = load_stopwords('/content/drive/My Drive/stopwords_en.txt')

# Initialize Porter stemmer
porter_stemmer = PorterStemmer()

# Initialize dictionaries to store counts for each channel ID and token frequencies
channel_id_counts = defaultdict(int)
token_frequencies = defaultdict(int)
english_counts = defaultdict(int)
channel_comments = defaultdict(list)  # Dictionary to store comments for each channel

# List to store stemmed and filtered comments
stem_com = []

# Loop through all channel IDs and text originals
for channel_id, text_original in zip(all_channel_ids, cd_comments):
    # Increment count for this channel ID
    channel_id_counts[channel_id] += 1

    # Tokenize text and filter out stop words
    tokens = nltk.word_tokenize(text_original.lower())  # Tokenize using NLTK
    tokens_filtered = [porter_stemmer.stem(token) for token in tokens if token not in stopwords and len(token) >= 3]  # Apply stemming and filter tokens with length >= 3

    # Count token frequencies
    for token in set(tokens_filtered):
        token_frequencies[token] += 1

    # Join tokens back into text
    text_without_stopwords = ' '.join(tokens_filtered)

    # Count English comments after stop words removal
    english_comments = detect_english_comments([text_without_stopwords])
    english_counts[channel_id] += len(english_comments)

    # Store comments for each channel
    channel_comments[channel_id].append(text_without_stopwords)

    # Append stemmed and filtered comment to the list
    stem_com.append(text_without_stopwords)

# Calculate the threshold frequency for rare tokens
total_channel_ids = len(set(all_channel_ids))
threshold_frequency = total_channel_ids * 0.01

# Filter out rare tokens
filtered_tokens = {token for token, freq in token_frequencies.items() if freq >= threshold_frequency}

# Filter out channel IDs with less than 15 English comments
filtered_channel_ids = [channel_id for channel_id, english_count in english_counts.items() if english_count >= 15]

# Print three comments for each filtered channel ID
for channel_id in filtered_channel_ids:
    count = channel_id_counts[channel_id]
    english_count = english_counts[channel_id]
    print(f"Channel ID: {channel_id}, Count: {count}, English Count: {english_count}")
    print("Comments:")
    for comment in channel_comments[channel_id][:3]:  # Print first three comments
        # Filter out rare tokens from the comment
        filtered_comment_tokens = [token for token in comment.split() if token in filtered_tokens]
        filtered_comment = ' '.join(filtered_comment_tokens)
        print(filtered_comment)
    print()  # Empty line for separation

# Now, stem_com contains the final stemmed and filtered comments
print("Stemmed and Filtered Comments:")
for comment in stem_com[:200]:  # Print first 50 comments as an example
    print(comment)

"""5.5

In this section, the script utilizes NLTK to extract meaningful bigrams from the preprocessed comments. It first preprocesses the text by tokenizing, converting to lowercase, and removing stopwords. Then, it creates a bigram collocation finder and filters out collocations occurring less than 10 times. Using Pointwise Mutual Information (PMI), it scores the collocations and extracts the top 200 meaningful bigrams. These bigrams are printed along with their PMI scores. Additionally, it retokenizes the extracted bigrams into unigrams and prints the resulting list of retokenized unigrams.
"""

''' Bigrams'''

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')


# Preprocessing function
def preprocess_text(text):
    # Tokenize text
    tokens = word_tokenize(text)

    # Remove punctuation and convert to lowercase
    tokens = [token.lower() for token in tokens if token.isalnum()]

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    return tokens

# Tokenize and preprocess each comment
tokenized_comments = [preprocess_text(comment) for comment in stem_com]

# Flatten the list of tokenized words
words = [word for comment_words in tokenized_comments for word in comment_words]

# Create a bigram collocation finder
bigram_finder = BigramCollocationFinder.from_words(words)

# Filter out collocations that occur less than 3 times
bigram_finder.apply_freq_filter(10)

# Score the collocations using Pointwise Mutual Information (PMI)
scored_collocations = bigram_finder.score_ngrams(BigramAssocMeasures.pmi)

# Extract the first 200 meaningful bigrams
meaningful_bigrams = [collocation for collocation, _ in scored_collocations[:200]]

# Print the first 200 meaningful bigrams
for bigram in meaningful_bigrams:
    print(bigram)

# Score the collocations using Pointwise Mutual Information (PMI)
scored_collocations = bigram_finder.score_ngrams(BigramAssocMeasures.pmi)

# Print the first 200 meaningful bigrams along with their PMI scores
for bigram, pmi_score in scored_collocations[:200]:
    print(f"Bigram: {bigram}, PMI Score: {pmi_score}")

''' Retokenizing Unigrams'''

# Initialize an empty list to store retokenized unigrams
retokenized_unigrams = []

# Retokenize each bigram into unigrams
for bigram in meaningful_bigrams:
    if isinstance(bigram, tuple) and len(bigram) == 2:  # Check if the bigram is a tuple with two elements
        unigram_tokens = nltk.word_tokenize(bigram[0])  # Tokenize the first word of the bigram
        unigram_tokens.extend(nltk.word_tokenize(bigram[1]))  # Tokenize the second word of the bigram and extend the list
        retokenized_unigrams.extend(unigram_tokens)  # Add unigrams to the list

# Print the retokenized unigrams
print("Retokenized Unigrams from Bigrams:")
print(retokenized_unigrams)

"""5.6

This section of the script extracts unigrams from the preprocessed comments stored in `stem_com`. It tokenizes each comment using NLTK's word tokenizer and collects unique tokens into a set to ensure uniqueness. Finally, it prints out the extracted unigrams.
"""

''' unigrams '''

# Initialize an empty set to store unique unigrams
unigrams = set()

# Tokenize each comment in stem_com and collect unique tokens
for comment in stem_com:
    tokens = nltk.word_tokenize(comment)  # Tokenize comment
    unigrams.update(tokens)  # Add unique tokens to the set

# Print the extracted unigrams
print("Extracted Unigrams:")
for unigram in unigrams:
    print(unigram)

"""5.7

This part of the script merges unigrams and retokenized unigrams into a single set called `vocabulary_set`. It then sorts the tokens, removes duplicates, and creates a dictionary called `token_index_mapping` to store each token along with its corresponding index. Finally, it prints out the first five token:index pairs and writes all token:index pairs to a text file named 'vocab.txt' in the specified format.
"""

#some code

''' output for vocab.txt'''



# Merge unigrams and retokenized unigrams into one set
vocabulary_set = set(unigrams).union(' '.join(bigram) for bigram in meaningful_bigrams)

# Sort the tokens and remove duplicates
sorted_vocabulary = sorted(set(vocabulary_set))

# Create a dictionary to store token:index pairs
token_index_mapping = {token: index for index, token in enumerate(sorted_vocabulary, start=0)}

# # Print the token:index pairs
# print("Token:Token_Index")
# for token, index in token_index_mapping.items():
#     print(f"{token}:{index}")

print("First 5 Token:Token_Index")
for token, index in sorted(token_index_mapping.items())[19834:19835]:
    print(f"{token}:{index}")

# Write token:index pairs to a text file
with open('/content/drive/My Drive/vocab.txt', 'w') as file:
    for token, index in sorted(token_index_mapping.items()):
        file.write(f"{token}:{index}\n")

print("Token:index pairs written to 'vocab.txt' file.")

"""At this stage, all reviews for each PID are tokenized and are stored as a value in the new dictionary (separetely for each day).

-------------------------------------

<div class="alert alert-block alert-warning">
    
### 6. Sparse Matrix <a class="anchor" name="whetev"></a>
"""

''' Do not run'''
''' Sparse representation Testing'''

from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

# Initialize CountVectorizer with token_index_mapping
vectorizer = CountVectorizer(vocabulary=token_index_mapping)

# Transform channel comments using CountVectorizer
sparse_matrix = vectorizer.fit_transform(eng_com)

# Get the token indices
token_indices = vectorizer.get_feature_names_out()

# Get the frequency matrix
frequency_matrix = sparse_matrix.toarray()

# Map token indices and frequencies back to each comment
for i, comment in enumerate(ci_comments):
    channel_id = filtered_channel_ids[i % len(filtered_channel_ids)]  # Cycle through channel IDs
    # Initialize frequency dictionary for the current comment
    frequency_dict = defaultdict(int)
    # Get the indices and frequencies for the current comment
    indices = sparse_matrix[i].indices
    frequencies = sparse_matrix[i].data
    # Map indices and frequencies to token_index_mapping
    for index, frequency in zip(indices, frequencies):
        token = token_indices[index]
        token_index = token_index_mapping.get(token)
        frequency_dict[token_index] += frequency
    # Print the sparse representation for the current comment
    print(f"Channel ID: {channel_id}, Comment: {comment}")
    for token_index, token_frequency in frequency_dict.items():
        print(f"{token_index}:{token_frequency}, ", end="")
    print()  # Move to the next line for the next comment

"""This final section of the script uses `CountVectorizer` from scikit-learn to transform the English comments into a sparse matrix representation based on the token indices obtained earlier. It then maps the token indices and frequencies back to each comment and prints out the sparse representation for each comment along with the corresponding channel ID.

Additionally, it writes the output lines to a text file named '037_countvec.txt'.
"""

'''Final Output'''
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

# Initialize CountVectorizer with token_index_mapping
vectorizer = CountVectorizer(vocabulary=token_index_mapping)

# Transform channel comments using CountVectorizer
sparse_matrix = vectorizer.fit_transform(eng_com)

# Get the token indices
token_indices = vectorizer.get_feature_names_out()

# Get the frequency matrix
frequency_matrix = sparse_matrix.toarray()

# Map token indices and frequencies back to each comment
for i in range(len(filtered_channel_ids)):  # Iterate over the first 10 comments
    channel_id = filtered_channel_ids[i % len(filtered_channel_ids)]  # Cycle through channel IDs
    # Initialize frequency dictionary for the current comment
    frequency_dict = defaultdict(int)
    # Get the indices and frequencies for the current comment
    indices = sparse_matrix[i].indices
    frequencies = sparse_matrix[i].data
    # Map indices and frequencies to token_index_mapping
    for index, frequency in zip(indices, frequencies):
        token = token_indices[index]
        token_index = token_index_mapping.get(token)
        frequency_dict[token_index] += frequency
    # Print the sparse representation for the current comment
    print(f"Channel ID: {channel_id}, Comment: {eng_com[i]}")
    for token_index, token_frequency in frequency_dict.items():
        print(f"{token_index}:{token_frequency}, ", end="")
    print()  # Move to the next line for the next comment

output_file_path = '/content/drive/My Drive/037_countvec.txt'

# Open the file in write mode
with open(output_file_path, 'w') as f:
    # Map token indices and frequencies back to each comment
    for i in range(len(filtered_channel_ids)):  # Iterate over the comments
        channel_id = filtered_channel_ids[i % len(filtered_channel_ids)]  # Cycle through channel IDs
        # Initialize a list to store token representations for the current comment
        token_repr_list = []
        # Get the indices and frequencies for the current comment
        indices = sparse_matrix[i].indices
        frequencies = sparse_matrix[i].data
        # Map indices and frequencies to token_index_mapping
        for index, frequency in zip(indices, frequencies):
            token = token_indices[index]
            token_index = token_index_mapping.get(token)
            token_repr_list.append(f"{token_index}:{frequency}")
        # Write the token representations for the current comment to the file
        f.write(f"{channel_id},{','.join(token_repr_list)}\n")


print("File Saved")

"""<div class="alert alert-block alert-warning">
    
### 4.3. Generate numerical representation<a class="anchor" name="bigrams"></a>

One of the tasks is to generate the numerical representation for all tokens in abstract.  .....
"""

#some code

#some code

"""Random descriptions and justification ..."""

#some code

"""At this stage, we have a dictionary of tokenized words, whose keys are indicative of....

-------------------------------------

#### Whatever else <a class="anchor" name="whatev1"></a>

<div class="alert alert-block alert-success">
    
## 5. Writing Output Files <a class="anchor" name="write"></a>

files need to be generated:
* Vocabulary list
* Sparse matrix (count_vectors)

This is performed in the following sections.

<div class="alert alert-block alert-warning">
    
### 5.1. Vocabulary List <a class="anchor" name="write-vocab"></a>

List of vocabulary should also be written to a file, sorted alphabetically, with their reference codes in front of them. This file also refers to the sparse matrix in the next file. For this purpose, .....
"""

#some code

"""<div class="alert alert-block alert-warning">
    
### 5.2. Sparse Matrix <a class="anchor" name="write-sparseMat"></a>

For writing sparse matrix for a paper, we firstly calculate the frequency of words for that paper ....
"""

#some code

"""-------------------------------------

<div class="alert alert-block alert-success">
    
## 6. Summary <a class="anchor" name="summary"></a>

The task at hand involves a comprehensive process of data extraction, normalization, and subsequent analysis. Initially, the focus is on extracting the 'textOriginal' fields from top-level comments, necessitating the removal of emojis and standardization of text to lowercase. Further refinement is achieved by isolating English comments from channels with a minimum of 15 such comments, determined through the langdetect library. Subsequently, a CSV file is generated, housing unique channel IDs alongside counts of all top-level comments and English comments. Following this, an intricate process of vocabulary and bigram generation unfolds. Tokenization using a specified regex pattern is conducted, coupled with the removal of context-independent and context-dependent stopwords, stemming, and the elimination of rare tokens and short-length tokens. Notably, the creation of meaningful bigrams is facilitated by the PMI measure. These unigrams and bigrams are then amalgamated, sorted alphabetically, and stored in 'vocab.txt'. Finally, sparse numerical representations are crafted, considering the generated bigram list, and saved in 'countvec.txt', adhering to the prescribed format of channel ID coupled with token indices and frequencies. This meticulous approach ensures a refined dataset poised for further analysis and exploration.

-------------------------------------

<div class="alert alert-block alert-success">
    
## 7. References <a class="anchor" name="Ref"></a>

## --------------------------------------------------------------------------------------------------------------------------
"""